---
title: "Cluster Analysis"
author: "Nithya Vembu"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#K-means Cluster Analysis

Week 9 Source - https://uc-r.github.io/kmeans_clustering

Clustering is the process of finding subgroups in a large dataset. This method is unsupervised since there is no target variable.

1. Replication Requirements

```{r warning=FALSE, results="hide", output = FALSE, message=FALSE}
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if (!require("magrittr")) install.packages("magrittr")
library(magrittr)
if (!require("cluster")) install.packages("cluster")
library(cluster)
if (!require("factoextra")) install.packages("factoextra")
library(factoextra)
if (!require("gridExtra")) install.packages("gridExtra")
library(gridExtra)
```

2. Data Preparation

The dataset used is USArrests which contains data of arrests for assault, muder and rape, per 100,000 residents in the US in 1973.
Missing values are first removed.
```{r}
df <- USArrests
df <- na.omit(df)
```

The data is scaled/standardized.

```{r}
df <- scale(df)
head(df)
```

3. Clustering Distance Measures

Grouping of observation requires computation of the distance or similarity between one another, which results in a dissimilarity or distance matrix. The most common distance methods are Euclidean and Manhattan distances. For correlation-based distances, Pearson correlation, Kendall correlation or Spearman correlation distances are used.

In R, distance can be measured using the get_dist and fviz_dist in the factoextra package. The default distance measured is EUclidean.

```{r}
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

4. K-Means Clustering

kmean() is used to do clustering in R. For this, the number of clusters to be created, along with the randomly selected objects as initial cluster centers.
In the example below, the data will be grouped into two clusters and will have 25 initial configurations.

```{r}
k2 <- kmeans(df, centers = 2, nstart = 25)
str(k2)
```

Printing the results will show that there are two clusters of sizes 20 and 30.
```{r}
k2
```

Using fviz_cluster, the data can be visualized. When there are more than two variables, fviz_cluster will perform principal component analysis (PCA) and display data for the two principal components accordingly.

```{r}
fviz_cluster(k2, data = df)
```


Pairwise scatterplots can also be used to visualize the clusters.

```{r}
df %>%
  as_tibble() %>%
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()
```

The number of specified clusters can be changed to examine the data more closely, as shown below.

```{r}
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

5. Determining Optimal Clusters

The three methods to obtain optimal clusters are 1. Elbow method, 2. Silhouette method and 3. Gap statistic.

The total within-cluster sum of squares are reduced, and is plotted. For elbow method, the number of clusters in the bed/knee will be the appropriate number of clusters.

```{r}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(df, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

```

The above steps of elbow method can be performed using just one line with the help of the fvuz_nbclust function.

```{r}
set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")
```

The average silhouette width to determine how well each object lies within the cluster is calculated in the Average Silhouette method, where the optimal number of clusters will be the one that maximizes the average silhouette range.

```{r}
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
  km.res <- kmeans(df, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(df))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")

```

The above steps of the average silhouette method can be performed using just one line with the help of the fvuz_nbclust function.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```

In gap statistic method, the total intracluster variation for different values of clusters is calculated.


```{r}
# compute gap statistic
set.seed(123)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)
```


6. Extracting Results

Finally, after obtaining optimal clusters, the results are extracted. Here, the number of optimals is 4.

```{r}
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 4, nstart = 25)
print(final)
```

The results can be visualized with fviz_cluster

```{r}
fviz_cluster(final, data = df)
```

Descriptive statistics can be done at the cluster level for further analysis.

```{r}
USArrests %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

# Hierarchical Cluster Analysis

Unlike the k-means clustering method, the number of cluster do not have to be specified beforehand. This clustering method results in a tree-based representation of observations called dendrogram, which makes it sometimes a better method than k-means.

1. R Package Requirements

```{r warning=FALSE, results="hide", output = FALSE, message=FALSE}
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if (!require("magrittr")) install.packages("magrittr")
library(magrittr)
if (!require("cluster")) install.packages("cluster")
library(cluster)
if (!require("factoextra")) install.packages("factoextra")
library(factoextra)
if (!require("dendextend")) install.packages("dendextend")
library(dendextend)
```

2. Hierarchical Clustering Algorithms

Hieratchical clustering can be of two types : agglomerative ad divisive. AGNES (Agglomerative Nesting) is a bottom-up approach where at each step, two single elements that are most similar are combined into clusters, while DIANA (Divise Analysis) is a top-down approach which begins at the root where everything is one big cluster, and at every step two dissimilar clusters are split up.

3. Data Preparation

The dataset USArrests is used again. The missing values are omitted and the data is scaled/standardized.

```{r}
df <- USArrests
df <- na.omit(df)
df <- scale(df)
head(df)

```


4. Hierarchical Clustering with R

Agglomerative HC can be performed with the hclust function which takes the dissimilarity values as input.

```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

The function agnes can be used instead of the above lines of code.

```{r}
# Compute with agnes
hc2 <- agnes(df, method = "complete")

# Agglomerative coefficient
hc2$ac
```

Using the function, multiple clustering methods can be evaluated. Ward's method proves to be the best for this dataset.

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac)
```

The dendogram can be visualized.

```{r}
hc3 <- agnes(df, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 
```


Divisive Hierarchical Clustering can be performed using the diana function. No specific methods are provided to this function like in agnes.

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(df)

# Divise coefficient; amount of clustering structure found
hc4$dc
## [1] 0.8514345

# plot dendrogram
pltree(hc4, cex = 0.6, hang = -1, main = "Dendrogram of diana")
```

4. Working with Dendrograms

In dendrograms, every leaf is an obervation. Similar observation combine into branches. The height between the different branch fusions help identity closeness between observations. The dendrogram can be split into subgroups using cutree.

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
sub_grp <- cutree(hc5, k = 4)

# Number of members in each cluster
table(sub_grp)
```

Observations from the original dataset can be added to the cutree output.

```{r}
USArrests %>%
  mutate(cluster = sub_grp) %>%
  head
```

The visualization of dendrograms can be customized as shown below.

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 4, border = 2:5)
```

Similar to k-means, fviz_cluster can be used examine the clusters closer.

```{r}
fviz_cluster(list(data = df, cluster = sub_grp))
```

cutree can be used with the agnes and diana functions.

```{r}
# Cut agnes() tree into 4 groups
hc_a <- agnes(df, method = "ward")
cutree(as.hclust(hc_a), k = 4)

# Cut diana() tree into 4 groups
hc_d <- diana(df)
cutree(as.hclust(hc_d), k = 4)
```

Using tanglegram(), the two dendrograms can be plotted side by side for comparison.

```{r}
# Compute distance matrix
res.dist <- dist(df, method = "euclidean")

# Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "complete")
hc2 <- hclust(res.dist, method = "ward.D2")

# Create two dendrograms
dend1 <- as.dendrogram (hc1)
dend2 <- as.dendrogram (hc2)

tanglegram(dend1, dend2)

```


The quality of alignment of two trees is measured using the entanglement function, where lower entanglement corresponds to a good alignment.

```{r}
dend_list <- dendlist(dend1, dend2)

tanglegram(dend1, dend2,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("entanglement =", round(entanglement(dend_list), 2))
  )
```

6. Determining Optimal Clusters

The three methods used in k-means can be used with  hierarchical clustering also.

Elbow method performed by changing the FUn argument in the fviz_nbclust function.

```{r}
fviz_nbclust(df, FUN = hcut, method = "wss")
```


Average Silhouette Method is similar to elbow method and only the method argument is changed.

```{r}
fviz_nbclust(df, FUN = hcut, method = "silhouette")
```


Gap Statistic is same also.

```{r}
gap_stat <- clusGap(df, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```
















